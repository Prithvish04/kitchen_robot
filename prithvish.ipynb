{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Assignment \n",
    "# Machine Learning for Robotics (RO47002) 2020/2021\n",
    "\n",
    "Group Number:\n",
    "\n",
    "Student 1 (name + student number):\n",
    "\n",
    "Student 2 (name + student number):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Note: Please remove this cell for the submission.*\n",
    "\n",
    "## Task Description\n",
    "In this final assignment, we revisit our robot from Practicum 2, which was able to pick up a pen after you trained it to recognize parts of the pen in its camera images. Now we want to teach the robot to be able to clean up a table after a meal. In particular, it needs to be able to pick up cutlery (forks, knives, spoons).  For that the robot needs to determine where the cutlery is located and where the handle is. The robot has a basic down-facing camera that it can place on top of the desk to inspect an area of interest. Your task is to design the machine learning method for this perception problem. To simplify the problem, we'll only consider a single type of cutlery (your choice of fork, spoon, or knife) and a single object visible in each camera picture.\n",
    "\n",
    "## Deliverables\n",
    "The deadline is Sunday October 25th, 23:59. Late submission is –1 grade point per day.\n",
    "\n",
    "The main deliverable is a Jupyter Notebook, integrating the report (markdown cells) and the code. Submission is again in the form of a single ZIP file that includes all files required to run the notebook and reproduce the results (collected images, annotation data, loadable parameters, auxiliary scripts, etc.). The notebook needs to be able to run within 10 minutes on a high-end PC, performing all steps (also including the hyperparameter optimization and training).  Unlike previous lab assignments, there are no autograded cells or asserts, but we will grade the notebook manually. Therefore, you are free to add cells as you see fit, as long as the required sections are still present in the notebook.\n",
    "\n",
    "## Grading Criteria\n",
    "Below you will find an outline of the sections that the notebook needs to contain and what we expect for each part. More specific requirements are listed there as well. The indicated number of points, out of a total of 100, should give you a rough indication on how much effort to put into each part.\n",
    "\n",
    "In general, we will not focus as much on the performance of the method you design, but rather the _level of understanding and argumentation about your design choices_. So, we are not only interested in WHAT you did, but will put a strong emphasis on your reasoning about the WHY. Try to synthesize rather than describing what you did step by step.\n",
    "\n",
    "### Quality of the Report (20 points)\n",
    "- Structure & Readability\n",
    " - Logical flow\n",
    " - Connection between parts\n",
    "- English\n",
    " - Do not use short forms, like \"isn't\", \"wouldn't\".\n",
    " - Do not use colloquial style, like \"a couple of\".\n",
    " - Spell check and proofread your report.\n",
    "- Level of detail\n",
    " - Strive for elegant, concise text - longer reports do not necessarily yield higher grades.\n",
    " - There is no need to re-explain theory. Assume that the target audience of the report has followed the course.\n",
    "- Figures & Tables\n",
    " - Choose figures/plots/tables carefully. Only include those that add to the story of the report. Do not put the burden on the reviewer to figure out which results you basing your conclusions on, but specifically refer (parts of) the specific table/plot/figure when needed.\n",
    " - When comparing two or more signals display them in one plot. Explain the colors / line types. The scale of the plots must be carefully chosen in order to clearly convey the information intended. Label properly the axes in graphs (variables and units).\n",
    "- Citations\n",
    " - If you use images, theory and methods beyond what was covered in the course, etc., always reference sources.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "*Note: Please do not include the requirements in the submission.*\n",
    "\n",
    "# Structure (inspired by the Machine Learning Project Checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame the Problem and Look at the Big Picture (10 points)\n",
    "- How do you frame the learning problem? Please treat the main learning problem as a supervised learning problem. But can you best express it as a classification problem, a regression problem, an image segmentation problem, etc.? Note that there is not one best answer to this question, and the task could be addressed in different ways. We want to know your motivation for your selected approach.\n",
    "- What are the runtime constraints, especially for predictions? How fast does the robot need to be able to process an image? What kind of platform do you assume (e.g., Arduino, Laptop, etc.)?\n",
    "- How should performance be measured? What is the minimum performance required? What are the objectives? What kind of loss function is appropriate?\n",
    "- What are promising algorithms?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data (15 points)\n",
    "- Collect your own dataset, i.e., take pictures of your chosen type of cutlery on a table. What was your protocol? How many pictures do you need? What kind of variations did you try to capture? What kind of potential variations are you trying to avoid to capture?\n",
    "- Annotate the data. Explain which options you considered and what you did in the end. If it is a custom annotation tool, include it in the submission, if you used an external tool, a link is sufficient\n",
    "- Sample a test set\n",
    "- *Note:* In contrast to Practicum 2, it is sufficient to just collect a single dataset and perform the training/testing split on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data (5 points)\n",
    "- Visualize the data\n",
    "- Study its properties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data (10 points)\n",
    "- Pre-process the data (e.g. down-sample, color channels)\n",
    "- Extract features (if needed by chosen algorithms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortlist Promising Models (15 points)\n",
    "- Compare at least 2 models. One of them needs to be a neural network, one of them needs to be not a neural network.\n",
    "- Perform dimensionality reduction (if needed)\n",
    "- Roughly tune those models\n",
    "- Evaluate the models in terms of performance, bias, variance, etc.\n",
    "- Pick one algorithm to develop further\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune the System (15 points)\n",
    "- Perform hyperparameter optimization (including pre-processing steps)\n",
    "- Evaluate the final model (similar to “Shortlist Promising Models” above)\n",
    "- Evaluate if your dataset was large and rich enough\n",
    "- Save the parameters of your best model to your harddrive (use pickle for sklearn or built-in save/load for keras), you will need to be able to reload your model without training in the next step. Be sure to include the saved parameters in your zip file so we can evaluate your best model too even without rerunning the notebook up to here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Present Your Solution (10 points)\n",
    "- Summarize your main decisions and insights\n",
    "- Create a stand-alone demo. I.e., a block of cells that can be run on its own. For that you will need to load your pre-trained best model you saved in the previous section, measure its performance on the test set, present results both in numbers and with illustrative examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Develop Your System (Bonus, max 10 points)\n",
    "- With this part you can make up for lost points in the main part. The maximum grade is still a 10. \n",
    "- Extend the system to work with either multiple types of cutlery, or multiple objects in a camera image, or different tables, or distractor objects, or different brands of your chosen type of cutlery, or a combination thereof.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame work \n",
    "Kitchen Robot ::: Bigger picture\n",
    "Task1: will be detecting an object (fork, spoon) (been done before) Assume this part in the beginning\n",
    "Task2: Figuring out the orientation of the object (we will do) in the notebook\n",
    "Task3: give the coordinates to the Robot (depending upon how much time we have)\n",
    "Task4: classifying what object it is from the features extracted\n",
    "Task5: Related to task 2 but we do not need to annotate images\n",
    "\n",
    "Plan to use the hardware\n",
    "Hardware we will use: Logitech camera, Laptop/BBB (Beagle Bone Black)\n",
    "\n",
    "Classification based problem (classify the pickup region) (spoon and fork will be in the middle)\n",
    "(knife--> not in the middle) middle of the handle\n",
    "\n",
    "Todo\n",
    "Explore the data -> show functions from pacticum2 and other practicums (prcticum6)\n",
    "\n",
    "Todo -> we already have patches\n",
    "explore -> other methods + greyscale image 100x100x3 -> 100x100\n",
    "explore -> down size image (not needed as we already extract patches)\n",
    "\n",
    "Extract feature -> if we use big patches will dimentionality reduction help(PCA eg)\n",
    "100x100 -> 10x10 or 100 (how does it affect the training)\n",
    "\n",
    "compare 2 models (neural net vs any other )\n",
    "Roughly tune those models (in case neural net) change hyperparameters\n",
    "Classifier -> check the web (if there are hyperparameters)\n",
    "\n",
    "dataset -> 7:3 or 8:2 (randomly divided ) (different lighting conditions)\n",
    "- Evaluate the models in terms of performance, bias, variance, etc.\n",
    "^ validate data 8 -> 4 folds  3:1 3 training 1 validation\n",
    "\n",
    "considering only 2\n",
    " multiple types of cutlery,\n",
    " different tables\n",
    " \n",
    "xin: spoon\n",
    "prithvish: knife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### a function which shows you the annotated points ###########\n",
    "def show_annotation(I, p1, p2):\n",
    "    plt.figure()\n",
    "    \n",
    "    # show the image\n",
    "    # plot point p1 as a green circle, with markersize 10, and label \"tip\"\n",
    "    # plot point p2 as a red circle, with markersize 10, and label \"end\"\n",
    "    # plot a line starts at one point and end at another. \n",
    "    # Use a suitable color and linewidth for better visualization\n",
    "    # Add a legend (tip, you can use the \"label\" keyword when you plot a point)\n",
    "    \n",
    "    plt.imshow(I)\n",
    "    plt_points_tip = plt.plot(p1[0], p1[1], 'ro', markersize = 10, c = 'g', label = 'tip')\n",
    "    plt_points_end = plt.plot(p2[0], p2[1], 'ro', markersize = 10, c = 'r', label = 'end')\n",
    "    plt_line = plt.plot([p1[0],p2[0]], [p1[1],p2[1]], linewidth = 2)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "######## a function which takes in samples around the annotated points ###########    \n",
    "def sample_points_around_pen(I, p1, p2):\n",
    "    Nu = 100 # uniform samples (will mostly be background, and some non-background)\n",
    "    Nt = 50 # samples at target locations, i.e. near start, end, and middle of pen\n",
    "    \n",
    "    target_std_dev = np.array(HALF_WIN_SIZE[:2])/3 # variance to add to locations\n",
    "\n",
    "    upoints = sample_points_grid(I)\n",
    "    idxs = np.random.choice(upoints.shape[0], Nu)\n",
    "    upoints = upoints[idxs,:]\n",
    "    \n",
    "    # sample around target locations\n",
    "    tpoints1 = np.random.randn(Nt,2)\n",
    "    tpoints1 = tpoints1 * target_std_dev + p1\n",
    "\n",
    "    tpoints2 = np.random.randn(Nt,2)\n",
    "    tpoints2 = tpoints2 * target_std_dev + p2\n",
    "\n",
    "    # sample over length pen\n",
    "    alpha = np.random.rand(Nt)\n",
    "    tpoints3 = p1[None,:] * alpha[:,None] + p2[None,:] * (1. - alpha[:,None])\n",
    "    tpoints3 = tpoints3 + np.random.randn(Nt,2) * target_std_dev\n",
    "    \n",
    "    # merge all points\n",
    "    points = np.vstack((upoints, tpoints1, tpoints2, tpoints3))\n",
    "    \n",
    "    # discard points close to border where we can't extract patches\n",
    "    points = remove_points_near_border(I, points)\n",
    "    \n",
    "    return points\n",
    "\n",
    "################ a function which makes labels from the points (tip, bottom, edge )\n",
    "def make_labels_for_points(I, p1, p2, points):\n",
    "    \"\"\" Determine the class label (as an integer) on point distance to different parts of the pen \"\"\"\n",
    "    num_points = points.shape[0]\n",
    "    \n",
    "    # for all points ....\n",
    "    \n",
    "    # ... determine their distance to tip of the pen\n",
    "    dist1 = points - p1\n",
    "    dist1 = np.sqrt(np.sum(dist1 * dist1, axis=1))\n",
    "    \n",
    "    # ... determine their distance to end of the pen\n",
    "    dist2 = points - p2\n",
    "    dist2 = np.sqrt(np.sum(dist2 * dist2, axis=1))\n",
    "\n",
    "    # ... determine distance to pen middle\n",
    "    alpha = np.linspace(0.2, 0.8, 100)\n",
    "    midpoints = p1[None,:] * alpha[:,None] + p2[None,:] * (1. - alpha[:,None]) \n",
    "    dist3 = scipy.spatial.distance_matrix(midpoints, points)\n",
    "    dist3 = np.min(dist3, axis=0)\n",
    "    \n",
    "    # the class label of a point will be determined by which distance is smallest\n",
    "    #    and if that distance is at least below `dist_thresh`, otherwise it is background\n",
    "    dist_thresh = WIN_SIZE[0] * 2./3.\n",
    "\n",
    "    # store distance to closest point in each class in columns\n",
    "    class_dist = np.zeros((num_points, 4))\n",
    "    class_dist[:,0] = dist_thresh\n",
    "    class_dist[:,1] = dist1\n",
    "    class_dist[:,2] = dist2\n",
    "    class_dist[:,3] = dist3\n",
    "    \n",
    "    # the class label is now the column with the lowest number\n",
    "    labels = np.argmin(class_dist, axis=1)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "############ function to extract a single patch from the image ########\n",
    "def get_patch_at_point(I, p, size):\n",
    "    # YOUR CODE HER\n",
    "    left = int(p[0]) - size[0]\n",
    "    down = int(p[1]) - size[0]\n",
    "    right = int(p[0]) + size[1]\n",
    "    up = int(p[1]) + size[1]\n",
    "    P = I[down:up, left:right, :]    \n",
    "    return P\n",
    "\n",
    "################## function to extract patches from the image ##########\n",
    "def extract_patches(I, p1, p2, size):\n",
    "    points = sample_points_around_pen(I, p1, p2)\n",
    "     \n",
    "    # determine the labels of the points\n",
    "    labels = make_labels_for_points(I, p1, p2, points)\n",
    "    \n",
    "    xs = []\n",
    "    for p in points:\n",
    "        P = get_patch_at_point(I, p, size)\n",
    "        x = patch_to_vec(P,size)\n",
    "        xs.append(x)\n",
    "    X = np.array(xs)\n",
    "\n",
    "    return X, labels, points\n",
    "\n",
    "######## count the number of labels per classs #############\n",
    "def count_classes(labels):\n",
    "    counts = np.array([0,0,0,0])\n",
    "    for i in labels:\n",
    "        counts[i] += 1\n",
    "    return counts\n",
    "\n",
    "def extract_multiple_images(Is, img_list,annots,size):\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    points = []\n",
    "    imgids = []\n",
    "\n",
    "    for idx in img_list:\n",
    "        I = Is[idx]\n",
    "        #print(idx)\n",
    "        #plt.figure()\n",
    "        #plt.imshow(I)\n",
    "        I_X, I_y, I_points = extract_patches(I, annots[idx,:2], annots[idx,2:], size)\n",
    "\n",
    "        classcounts = count_classes(I_y)\n",
    "        #print(f'image {idx}, class count = {classcounts}')\n",
    "\n",
    "        Xs.append(I_X)\n",
    "        ys.append(I_y)\n",
    "        points.append(I_points)\n",
    "        imgids.append(np.ones(len(I_y),dtype=int)*idx)\n",
    "\n",
    "    Xs = np.vstack(Xs)\n",
    "    ys = np.hstack(ys)\n",
    "    points = np.vstack(points)\n",
    "    imgids = np.hstack(imgids)\n",
    "    \n",
    "    return Xs, ys, points, imgids\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    'background', # class 0\n",
    "    'tip',        # class 1\n",
    "    'end',        # class 2\n",
    "    'middle'      # class 3\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "############## a function for plotting the samples on the image\n",
    "def plot_samples(Ps, labels,FEAT_SIZE,nsamples):\n",
    "    uls = np.unique(labels)\n",
    "    nclasses = len(uls)\n",
    "    \n",
    "    plt.figure(figsize=(10,4))\n",
    "    \n",
    "    for lidx, label in enumerate(uls):\n",
    "        idxs = np.where(labels == label)[0]\n",
    "        idxs = np.random.choice(idxs, nsamples, replace=False)\n",
    "        \n",
    "        for j, idx in enumerate(idxs):\n",
    "            P = Ps[idx,:]\n",
    "            P = P.reshape(FEAT_SIZE)\n",
    "            \n",
    "            plt.subplot(nclasses, nsamples, lidx*nsamples+j+1)\n",
    "            plt.imshow(P, clim=(0,1))\n",
    "            plt.axis('off')\n",
    "            plt.title('label: %d' % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## extracting annotation and images from the folder ###################\n",
    "annot_filename_1 = os.path.join('images/lab_partner1', 'annots.npy')\n",
    "annots_1 = pickle.load(open(annot_filename_1, 'rb'))\n",
    "filenames_1 = list_images('images/lab_partner1')\n",
    "N_1 = len(filenames_1)\n",
    "Is_1 = [plt.imread(filename) for filename in filenames_1]\n",
    "\n",
    "annot_filename_2 = os.path.join('images/lab_partner2', 'annots.npy')\n",
    "annots_2 = pickle.load(open(annot_filename_2, 'rb'))\n",
    "filenames_2 = list_images('images/lab_partner2')\n",
    "N_2 = len(filenames_2)\n",
    "Is_2 = [plt.imread(filename) for filename in filenames_2]\n",
    "\n",
    "Is = np.concatenate((Is_1,Is_2))\n",
    "annots = np.concatenate((annots_1,annots_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### following cell contains the sample points and label extraction from the annotated points and image ####\n",
    "######################### in short it is for important feature extraction ##################################\n",
    "\n",
    "\n",
    "img_idx=0\n",
    "I = Is[img_idx]\n",
    "p1 = annots[img_idx,:2].copy() # point 1, tip of the pen\n",
    "p2 = annots[img_idx,2:].copy() # point 2, end of the pen\n",
    "points = sample_points_around_pen(I, p1, p2)\n",
    "labels = make_labels_for_points(I, p1, p2, points)\n",
    "#plt.figure(figsize=(10,12))\n",
    "#plt.imshow(I)\n",
    "#plot_labeled_points(points, labels)\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ a loop which iterates over all the samples and plots the anotation and sample points######\n",
    "########## following loop is just to confirm if annotation and sampling work #################\n",
    "\n",
    "#for img_idx in range(N_2):\n",
    "#    I = Is_2[img_idx]\n",
    "#    p1 = annots_2[img_idx,:2].copy() # point 1, tip of the pen\n",
    "#    p2 = annots_2[img_idx,2:].copy() # point 2, end of the pen\n",
    "#    show_annotation(I, p1, p2)\n",
    "#    points = sample_points_around_pen(I, p1, p2)\n",
    "#    labels = make_labels_for_points(I, p1, p2, points)\n",
    "#    plt.figure(figsize=(10,12))\n",
    "#    plt.imshow(I)\n",
    "#    plot_labeled_points(points, labels)\n",
    "#    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(23)\n",
    "indices = list(range(0, len(Is)))\n",
    "random.shuffle(indices)\n",
    "train_imgs=indices[:45]\n",
    "test_imgs=indices[45:]\n",
    "\n",
    "FEAT_SIZE = (9,9,3)\n",
    "#print(Is_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Is' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-043baa1a683c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgids_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_multiple_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFEAT_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgids_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_multiple_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFEAT_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#plot_samples(X_train, y_train, FEAT_SIZE, nsamples = 12)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Is' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, y_train, points_train, imgids_train = extract_multiple_images(Is,train_imgs, annots, FEAT_SIZE)\n",
    "X_test, y_test, points_test, imgids_test = extract_multiple_images(Is, test_imgs, annots, FEAT_SIZE)\n",
    "#plot_samples(X_train, y_train, FEAT_SIZE, nsamples = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_predict= rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train training data with neural networks and test the model\n",
    "\n",
    "num_input_units, NNH_1, NNH_2, num_logits = config_layers(X_train, 173, 67, y_train)\n",
    "\n",
    "image_model = keras.models.Sequential()\n",
    "image_model.add(Dense(NNH_1, input_dim=num_input_units, activation='relu'))\n",
    "# YOUR CODE HERE\n",
    "image_model.add(Dense(NNH_2, activation='relu'))\n",
    "#raise NotImplementedError()\n",
    "image_model.add(Dense(num_logits, activation='softmax'))\n",
    "\n",
    "# compile the keras model\n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "# YOUR CODE HERE\n",
    "image_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "validation_split = 0.33\n",
    "epochs = 30\n",
    "batch_size = 10\n",
    "y_train_one_hot = one_hot_encoding(y_train)\n",
    "\n",
    "image_model.fit(X_train, y_train_one_hot, validation_split=validation_split,\\\n",
    "                    epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "y_test_one_hot = one_hot_encoding(y_test)\n",
    "_, accuracy = image_model.evaluate(X_test, y_test_one_hot)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
